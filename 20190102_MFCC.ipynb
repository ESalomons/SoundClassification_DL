{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zelfde experiment als eerder, alleen ditmaal met mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io.wavfile as wv\n",
    "import scipy.signal as sig\n",
    "import wave\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "\n",
    "from util import util\n",
    "from util import WavFileParts\n",
    "from util.logUtil import LOG, LOG_HEADER\n",
    "from util.confusionMatrix import ConfusionMatrix\n",
    "\n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### globale settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['music', 'voice', 'environment']\n",
    "\n",
    "macDir = '/Volumes/SAA_DATA/datasets/'\n",
    "winDir = 'E:/SAA_DATA/'\n",
    "osDir = winDir\n",
    "recordingDir = osDir + '/localizationRecordings'\n",
    "\n",
    "if osDir == winDir:\n",
    "    storageFolder = 'E:/SAA_DATA/storedData/'\n",
    "else:\n",
    "    storageFolder = '/Users/etto/Desktop/storedData/'\n",
    "\n",
    "baseSrcDir = osDir + 'localizationFiles/20171025AllExtractionsMic4'\n",
    "orgWavDirs1 = ['G428_0.0_1.4',\n",
    "              'G527_0.5_1.4',\n",
    "              'Studio_2.0_4.2'\n",
    "              ]\n",
    "\n",
    "orgWavDirs2 = ['G428_2.1_2.4',\n",
    "              'G527_1.2_5.8',\n",
    "              'Studio_3.0_2.0'\n",
    "              ]\n",
    "\n",
    "orgsG428 = ['G428_0.0_1.4','G428_2.1_2.4']\n",
    "orgsG527 = ['G527_0.5_1.4','G527_1.2_5.8']\n",
    "orgsStudio = ['Studio_2.0_4.2','Studio_3.0_2.0']\n",
    "\n",
    "chunksBaseDir = 'chunks'\n",
    "rooms = ['Studio', 'G428', 'G527']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility functies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSoundChunksDynamic(moduleString):\n",
    "    chunks = importlib.import_module(moduleString).soundChunks\n",
    "    wfPts = []\n",
    "    for jsonString in chunks:\n",
    "        wfPts.append(WavFileParts.WavFilePartFromJson(jsonString))\n",
    "    return wfPts\n",
    "\n",
    "def timeFunction(func):\n",
    "    \"\"\"\n",
    "    Aanroep: bijv. fpc = timeFunction(lambda: getFilesPerCategory(srcDir))\n",
    "    \"\"\"\n",
    "    startTime = datetime.now()\n",
    "    print('Start: ' + startTime.strftime('%H:%M:%S') + '\\n=================')\n",
    "\n",
    "    res = func()\n",
    "    \n",
    "    endTime = datetime.now()\n",
    "    print('\\n=================\\nEnd: ' + endTime.strftime('%H:%M:%S'))\n",
    "    print('Time taken: '),\n",
    "    print(endTime - startTime)\n",
    "    print()\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def storeTestData(allMfccs, allClasses, storageName, keyName):\n",
    "    filename = storageFolder + storageName + '_MFCC.hd5'\n",
    "    df = pd.DataFrame(allMfccs)\n",
    "    df.to_hdf(path_or_buf=filename, key='mfccs_' + keyName)\n",
    "\n",
    "    df = pd.DataFrame(allClasses)\n",
    "    df.to_hdf(path_or_buf=filename, key='classes_' + keyName)\n",
    "\n",
    "def retrieveTestData(storageName, NFFT, keyName):\n",
    "    filename = storageFolder + storageName + '_MFCC.hd5'\n",
    "    allMfccsDf = pd.read_hdf(path_or_buf=filename, key='mfccs_' + keyName)\n",
    "    classesDf = pd.read_hdf(path_or_buf=filename, key='classes_' + keyName)\n",
    "    return allMfccsDf.values, classesDf.values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functies tbv trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maakt een dictionary aan; per categorie alle files (volledig pad) uit de srcDir\n",
    "# srcDir is een van de orgWavDirs, bijvoorbeeld\n",
    "#    localizationFiles/20171025AllExtractionsMic4/G428_0.0_1.4\n",
    "def getFilesPerCategory(srcDir):\n",
    "    filesPerCategory = {}\n",
    "    for catDirLong in glob.glob(srcDir + '/*'):\n",
    "        catDir = catDirLong.replace('\\\\', '/')\n",
    "        catDir = catDir.replace(srcDir + '/', '')\n",
    "\n",
    "        filesPerCategory[catDir] = []\n",
    "        for filename in glob.glob(catDirLong + '/*'):\n",
    "            filename = filename.replace('\\\\','/')\n",
    "            filesPerCategory[catDir].append(filename)\n",
    "    return filesPerCategory\n",
    "\n",
    "def getFilesPerCatFromMultipleDirs(srcDirs, srcDirsBase=''):\n",
    "    filesPerCat = {}\n",
    "    for dirName in srcDirs:\n",
    "        srcDir = srcDirsBase + '/' + dirName\n",
    "        fpcNw = getFilesPerCategory(srcDir)\n",
    "        if not filesPerCat:\n",
    "            filesPerCat = fpcNw\n",
    "        else:\n",
    "            for key in filesPerCat:\n",
    "                filesPerCat[key] += fpcNw[key]\n",
    "    return filesPerCat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maakt een dictionary aan; per categorie de spectrogrammen\n",
    "def getMFCCsFromFilesPerCategory(filesPerCategory):\n",
    "    mfccs = {}\n",
    "    nfft = 1024\n",
    "    for clz in classes:\n",
    "        mfccs[clz] = []\n",
    "        for filename in filesPerCategory[clz]:\n",
    "            fs, signal = wv.read(filename)\n",
    "            winlen = nfft/fs \n",
    "            mfcc_feat = mfcc(signal, samplerate=fs, winlen=winlen, nfft=nfft)\n",
    "            mfccs[clz].append(mfcc_feat)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassLengths(mfccsPerCat):\n",
    "    clzLengths = {}\n",
    "    for clz in classes:\n",
    "        clzLengths[clz] = sum([np.shape(lst)[0] for lst in mfccsPerCat[clz]])\n",
    "    return clzLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verwacht invoer van getMFCCsFromFilesPerCategory\n",
    "# levert traindata op (X_train en Y_train)\n",
    "def createTrainDataFromMFCCs(mfccsPerCat, clzLengths):\n",
    "    X_train = np.concatenate(mfccsPerCat[classes[0]], axis=0)\n",
    "    for i in range(1, len(classes)):\n",
    "        nwMFCCs = np.concatenate(mfccsPerCat[classes[i]], axis=0)\n",
    "        X_train = np.concatenate((X_train,nwMFCCs), axis=0)\n",
    "    \n",
    "    # one-hot encoding voor Y_train\n",
    "    nrFiles = clzLengths[classes[0]]\n",
    "    Y_train = np.array((np.ones(nrFiles),np.zeros(nrFiles), np.zeros(nrFiles))).T\n",
    "\n",
    "    nrFiles = clzLengths[classes[1]]\n",
    "    Y_train_nw = np.array((np.zeros(nrFiles), np.ones(nrFiles), np.zeros(nrFiles))).T\n",
    "    Y_train = np.concatenate((Y_train, Y_train_nw),axis=0)\n",
    "\n",
    "    nrFiles = clzLengths[classes[2]]\n",
    "    Y_train_nw = np.array((np.zeros(nrFiles), np.zeros(nrFiles), np.ones(nrFiles))).T\n",
    "    Y_train = np.concatenate((Y_train, Y_train_nw),axis=0)\n",
    "    \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layersizes):\n",
    "    # create model\n",
    "    nr_inputs = 13\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layersizes[0], input_dim=nr_inputs, activation='relu'))\n",
    "    for lsize in layersizes[1:]:\n",
    "        model.add(Dense(lsize, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelFileName(modelFilePath, baseModelFilename, layers, nrEpochs):\n",
    "    modelFilename = modelFilePath + baseModelFilename\n",
    "    for lsize in layers:\n",
    "        modelFilename = '{}_{}'.format(modelFilename, lsize)\n",
    "    modelFilename += 'ep{}'.format(nrEpochs)\n",
    "    modelFilename += '.hd5'\n",
    "    return modelFilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelName(baseModelFilename, layers, nrEpochs):\n",
    "    modelFilename = baseModelFilename\n",
    "    for lsize in layers:\n",
    "        modelFilename = '{}_{}'.format(modelFilename, lsize)\n",
    "    modelFilename += 'ep{}'.format(nrEpochs)\n",
    "    return modelFilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, layers, nrEpochs, modelFilePath, baseModelFilename, batch_size=None):\n",
    "    soundModel = create_model(layers)\n",
    "    history = timeFunction(lambda: soundModel.fit(X_train,Y_train, epochs=nrEpochs, shuffle=True, verbose=1, batch_size=batch_size))\n",
    "    soundModel.save(getModelFileName(modelFilePath, baseModelFilename, layers, nrEpochs))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, Y_train, realTrainClasses, layers, nrEpochs, modelFilePath, baseModelFilename, batch_size=None):\n",
    "    soundModel = create_model(layers)\n",
    "    history = timeFunction(lambda: soundModel.fit(X_train,Y_train, epochs=nrEpochs, shuffle=True, verbose=1, batch_size=batch_size))\n",
    "    soundModel.save(getModelFileName(modelFilePath, baseModelFilename, layers, nrEpochs))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    evaluate_model(X_train, realTrainClasses,layers, nrEpochs, modelFilePath, baseModelFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, realClasses, layers, nrEpochs, modelFilePath, baseModelFilename):\n",
    "    soundModel = load_model(getModelFileName(modelFilePath, baseModelFilename, layers, nrEpochs))\n",
    "\n",
    "    # predicted classes\n",
    "    predictions = soundModel.predict(X_test)\n",
    "    predClasses = predictions.argmax(axis=1)\n",
    "\n",
    "    matrix = ConfusionMatrix(classes)\n",
    "    for vals in zip(realClasses, predClasses):\n",
    "        matrix.add(int(vals[0]), int(vals[1]), 1)\n",
    "    LOG(matrix.toString(),True)\n",
    "    LOG('', True)\n",
    "    LOG(matrix.toF1String(), True)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_per_epoch(X_train, Y_train, realClasses, layers, nrEpochs, modelFilePath, baseModelFilename, batch_size=None):\n",
    "    soundModel = create_model(layers)\n",
    "    for epNr in range(1, nrEpochs+1):\n",
    "        LOG('\\n*****************\\n* Epoch nr {}\\n*****************\\n'.format(epNr), True)\n",
    "        soundModel.fit(X_train,Y_train, epochs=1, shuffle=True, verbose=1, batch_size=batch_size)\n",
    "        soundModel.save(getModelFileName(modelFilePath, baseModelFilename, layers, epNr))\n",
    "        evaluate_model(X_train, realClasses, layers, epNr, modelFilePath, baseModelFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functies tbv testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndStoreTestData(wavFileParts, baseDir, fileDate, micNr, storeFilename, keyName):\n",
    "    nfft = 1024\n",
    "    allMfccs = np.array([])\n",
    "    allClasses = np.array([])\n",
    "\n",
    "    for wfPt in wavFileParts: #type: WavFilePart\n",
    "        if not 'Gunshot' in wfPt.getSoundType():\n",
    "            filename = baseDir + '/{:d}_{:d}_mono{:d}.wav'.format(fileDate, wfPt.fileNr, micNr)\n",
    "            fs, signal = wv.read(filename)\n",
    "            winlen = nfft / fs\n",
    "            classNr = classes.index(wfPt.getSoundType().lower())\n",
    "            for soundChunk in wfPt.getSoundChunks(micNr):\n",
    "                startFrame = int(soundChunk[0] * fs)\n",
    "                endFrame = int(soundChunk[1] * fs)\n",
    "\n",
    "                sigChunk = signal[startFrame: endFrame]\n",
    "                mfcc_feat = mfcc(signal, samplerate=fs, winlen=winlen, nfft=nfft)\n",
    "                if len(allMfccs) == 0:\n",
    "                    allMfccs = mfcc_feat\n",
    "                else:\n",
    "                    allMfccs = np.append(allMfccs, mfcc_feat, axis=0)\n",
    "                allClasses = np.append(allClasses, classNr * np.ones(len(mfcc_feat)))       \n",
    "    storeTestData(allMfccs, allClasses, storeFilename, keyName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataFromFolders(orgWavDirs):\n",
    "    fpc = getFilesPerCatFromMultipleDirs(orgWavDirs, baseSrcDir)\n",
    "    mfccs = getMFCCsFromFilesPerCategory(fpc)\n",
    "    clzLengths = getClassLengths(mfccs)\n",
    "    X_train, Y_train = createTrainDataFromMFCCs(mfccs, clzLengths)\n",
    "    # real train classes\n",
    "    realTrainClasses = np.concatenate((np.zeros(clzLengths[classes[0]]), \n",
    "                                  np.ones(clzLengths[classes[1]]), \n",
    "                                  2*np.ones(clzLengths[classes[2]])))\n",
    "    return X_train, Y_train, realTrainClasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModellen(modelFilePath, layerss, micNr, testFileNames, showOverall=True):\n",
    "\n",
    "    # test data\n",
    "    keyname = 'mic{}'.format(micNr)\n",
    "\n",
    "    for layers in layerss:\n",
    "        for nrEpochs in nrsEpochs:\n",
    "            overallMatrix = ConfusionMatrix(classes)\n",
    "            for testFileName in testFileNames:\n",
    "                testSpecs, testClasses = retrieveTestData(testFileName, keyname)\n",
    "\n",
    "                LOG_HEADER(getModelName(baseModelFilename, layers, nrEpochs) \n",
    "                           + '\\n# ' + keyname\n",
    "                           + '\\n# ' + testFileName,\n",
    "                           True)\n",
    "                matrix = evaluate_model(testSpecs, testClasses, layers, nrEpochs, modelFilePath, baseModelFilename)\n",
    "                overallMatrix.addMatrix(matrix.normalizedCopy())\n",
    "            \n",
    "            if showOverall:\n",
    "                overallMatrix = overallMatrix.normalizedCopy()\n",
    "                LOG_HEADER(getModelName(baseModelFilename, layers, nrEpochs) \n",
    "                           + '\\n# ' + keyname\n",
    "                           + '\\n# ' + 'testData_Overall',\n",
    "                           True)\n",
    "                LOG(overallMatrix.toString(),True)\n",
    "                LOG('', True)\n",
    "                LOG(overallMatrix.toF1String(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(modelFilePath, layers, micNr, testFileNames, nrEpochs, showOverall=True):\n",
    "\n",
    "    # test data\n",
    "    keyname = 'mic{}'.format(micNr)\n",
    "\n",
    "    overallMatrix = ConfusionMatrix(classes)\n",
    "    for testFileName in testFileNames:\n",
    "        testSpecs, testClasses = retrieveTestData(testFileName, keyname)\n",
    "\n",
    "        LOG_HEADER(getModelName(baseModelFilename, layers, nrEpochs) \n",
    "                   + '\\n# ' + keyname\n",
    "                   + '\\n# ' + testFileName,\n",
    "                   True)\n",
    "        matrix = evaluate_model(testSpecs, testClasses, layers, nrEpochs, modelFilePath, baseModelFilename)\n",
    "        overallMatrix.addMatrix(matrix.normalizedCopy())\n",
    "\n",
    "    if showOverall:\n",
    "        overallMatrix = overallMatrix.normalizedCopy()\n",
    "        LOG_HEADER(getModelName(baseModelFilename, layers, nrEpochs) \n",
    "                   + '\\n# ' + keyname\n",
    "                   + '\\n# ' + 'testData_Overall',\n",
    "                   True)\n",
    "        LOG(overallMatrix.toString(),True)\n",
    "        LOG('', True)\n",
    "        LOG(overallMatrix.toF1String(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainen van model obv: alles G428, G527 en Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train voor alle lagen\n",
    "baseModelFilename = '20190103_MFCC'\n",
    "modelFilePath = storageFolder\n",
    "logPrefix = 'Alle orgs'\n",
    "orgWavDirs = orgsG428 + orgsG527 + orgsStudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, realTrainClasses = getTrainDataFromFolders(orgWavDirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643318, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643318, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643318,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(realTrainClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "#\n",
      "# maak mfccs voor testdata G428\n",
      "#\n",
      "####################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Etto\\Anaconda2\\envs\\py36DL\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klaar: mfccs voor testdata G428\n"
     ]
    }
   ],
   "source": [
    "# Maak test data\n",
    "LOG_HEADER('maak mfccs voor testdata G428', True)\n",
    "wvPts = readSoundChunksDynamic('chunks.G428.soundChunks')\n",
    "baseDir = recordingDir + '/20170221'\n",
    "fileDate = 170221\n",
    "filename = 'testData_G428_MFCC'\n",
    "for micNr in [1,2,3,4]:\n",
    "    keyname = 'mic{}'.format(micNr)\n",
    "    createAndStoreTestData(wvPts, baseDir, fileDate, micNr, filename, keyname)\n",
    "\n",
    "LOG('Klaar: mfccs voor testdata G428', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "#\n",
      "# maak mfccs voor testdata G527}\n",
      "#\n",
      "####################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Etto\\Anaconda2\\envs\\py36DL\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klaar: mfccs voor testdata G527\n"
     ]
    }
   ],
   "source": [
    "# Maak test data\n",
    "LOG_HEADER('maak mfccs voor testdata G527}', True)\n",
    "wvPts = readSoundChunksDynamic('chunks.G527.soundChunks')\n",
    "baseDir = recordingDir + '/20170221'\n",
    "fileDate = 170221\n",
    "filename = 'testData_G527_MFCC'\n",
    "for micNr in [1,2,3,4]:\n",
    "    keyname = 'mic{}'.format(micNr)\n",
    "    createAndStoreTestData(wvPts, baseDir, fileDate, micNr, filename, keyname)\n",
    "\n",
    "LOG('Klaar: mfccs voor testdata G527', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "#\n",
      "# maak mfccs voor testdata Studio\n",
      "#\n",
      "####################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Etto\\Anaconda2\\envs\\py36DL\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klaar: mfccs voor testdata Studio\n"
     ]
    }
   ],
   "source": [
    "LOG_HEADER('maak mfccs voor testdata Studio', True)\n",
    "wvPts = readSoundChunksDynamic('chunks.Studio.soundChunks')\n",
    "baseDir = recordingDir + '/20171011'\n",
    "fileDate = 170816\n",
    "filename = 'testData_Studio_MFCC'\n",
    "for micNr in [1,2,3,4]:\n",
    "    keyname = 'mic{}'.format(micNr)\n",
    "    createAndStoreTestData(wvPts, baseDir, fileDate, micNr, filename, keyname)\n",
    "\n",
    "LOG('Klaar: mfccs voor testdata Studio', True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layersList = [[100, 20], [400, 250, 100, 20], [400, 300, 200, 100, 50, 20, 10], \n",
    "              [450, 400, 350, 300, 250, 200, 150, 100, 50, 21]]\n",
    "nrEpochs = 3\n",
    "nrsEpochs = range(1, nrEpochs + 1)\n",
    "#nrsEpochs = [nrEpochs]\n",
    "bSize = 128"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for layers in layersList:\n",
    "    LOG_HEADER(logPrefix + ', lagen: {}'.format(str(layers)), True)\n",
    "    train_and_evaluate_per_epoch(X_train, Y_train, realTrainClasses, \n",
    "                                 layers, nrEpochs, modelFilePath, baseModelFilename, NFFT,\n",
    "                                batch_size=bSize)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Test alle modellen\n",
    "testModellen(modelFilePath=storageFolder, \n",
    "             layerss=layersList, \n",
    "             NFFT=NFFT,\n",
    "             micNr=4, \n",
    "             testFileNames=['testData_G428', 'testData_G527', 'testData_Studio'], \n",
    "             showOverall=True)                "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from parseExperimentResults import parseRedoExperiment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "experimentId='20181215_NFFT2048'\n",
    "filePattern='20181215_NFFT2048'\n",
    "parseRedoExperiment(experimentId, filePattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "251px",
    "left": "448.415px",
    "right": "20px",
    "top": "98.9773px",
    "width": "579px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
